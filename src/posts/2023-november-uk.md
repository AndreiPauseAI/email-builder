---
title: PauseAI protest @ Bletchley Park - November 1st
description: We are organizing a protest at Bletchley Park, during the AI Safety Summit
---

- [Facebook event](https://www.facebook.com/events/347499967619516/347499967619516)
- [Sign up](https://www.mixily.com/event/4419031774197158693)

## Why we protest

AI is rapidly becoming more powerful, far faster than virtually any AI scientist has predicted.
Billions are being poured into AI capabilities, and the results are staggering.
New models are [outperforming humans](/sota) in a lot of domains.
As capabilities increase, so do the [risks](/risks).
Scientists are even [warning](https://www.safe.ai/statement-on-ai-risk) that AI might [end up destroying humanity](/xrisk).
This dire outcome not only seems possible, but also likely, as the average probability estimates for these outcomes [range from 14% to 40%](/polls-and-surveys).

On November 1st and 2nd, the very first AI Safety Summit will be held in the UK.
The perfect opportunity to set the first steps towards sensible international AI safety regulation.

However, it does not seem like the ones in charge are feeling [how little time we may have](/urgency).
The organiser and PM's Representative for the AI Safety Summit, Matt Clifford, has [stated](https://twitter.com/PauseAI/status/1709845853668553065) that “Pausing AI development now would be premature”, and that he [does not expect](https://twitter.com/matthewclifford/status/1708819574739587356) “hard controls” from the summit.
The AI safety paper released last week [seems to suggest](https://twitter.com/PauseAI/status/1717474950557090151) that the UK is confident we'll have many years to prepare for AGI.
The UK is relying on estimates from last year, before ChatGPT was released.
On Metaculus, the prediction [dropped](https://metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) from 2047 in March 2022 to 2026 today.
That's a 20-year drop in 18 months!

**We need our leaders to err on the side of caution, and implement a pause RIGHT NOW.**

## What we ask

- **Policy makers**: Don't allow companies to build a superintelligence. Regulations and hardware restrictions should apply before training has started as it is very difficult to control dissemination once a new capability has been achieved. . We cannot allow companies to train potentially world-ending AI models. Writing legislation is hard, and it takes time, but we may not have that long, so work as if your life depends on it. Because it does.
- **Companies**: Many of you are scared of what AI can do, but you're locked in a race. So be vocal about supporting a pause in principle. If you sign statements that this technology could kill us all, show the world that you'd prefer not to build it if it was a viable option.
- **Summit invitees**: Prioritize safety over economic growth. We know AI can make our countries richer, but that's not why you're summoned here. Be the adult in the room.

For our entire proposal, see [here](/proposal).
<!--
## Press Release

_FOR RELEASE ON NOVEMBER 1ST 2023_

### Protest During AI Safety Summit Calls For a Halt to Dangerous AI Development

**November 1st:** [**PauseAI**](https://pauseai.info/) **is holding a** [**protest in Bletchley Park, during the AI Safety Summit**](https://pauseai.info/2023-oct) **urging policy makers and AI Safety Summit attendees to ban the creation of a superintelligent AI as soon as possible.**

In March this year many notable experts signed [a letter](https://futureoflife.org/open-letter/pause-giant-ai-experiments/#:~:text=We%20call%20on%20all%20AI,more%20powerful%20than%20GPT%2D4.&text=AI%20systems%20with%20human%2Dcompetitive,acknowledged%20by%20top%20AI%20labs.) calling for a six-month halt on the development of their frontier AI models. In May, hundreds of AI scientists signed [a statement](https://www.safe.ai/statement-on-ai-risk) saying “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.”

Recent polls in [the US](https://www.vox.com/future-perfect/2023/9/19/23879648/americans-artificial-general-intelligence-ai-policy-poll) and [the UK](https://inews.co.uk/news/politics/voters-deepfakes-ban-ai-intelligent-humans-2708693) have shown that a large majority of people want the government to step in and prevent a superintelligent AI from being built. As of now, [no drafts](https://twitter.com/PauseAI/status/1706605169608159458) are being proposed that would do this.

On November 1st and 2nd, the very first AI Safety Summit will take place at Bletchley Park, UK.
The summit will be attended by leading AI scientists, policy makers, and industry leaders.
This marks a unique chance to set the first steps towards international AI safety regulation.
However the UK is not planning to use this opportunity to implement strong AI regulation.
The organiser and PM's Representative for the AI Safety Summit, Matt Clifford, has [stated](https://twitter.com/PauseAI/status/1709845853668553065) that “Pausing AI development now would be premature”, and that he [does not expect](https://twitter.com/matthewclifford/status/1708819574739587356) “hard controls” from the summit.

“We’re glad the UK is spearheading AI safety and showing international leadership”, says Joep Meindertsma, director of PauseAI. ”But we’re not seeing the level of urgency that it deserves. In 2020, forecasters [predicted](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) the arrival of human-level AI in 2055. Today the [average prediction](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) is 2026. We cannot risk disaster by underestimating the rate of progress. We need our politicians to err on the side of caution. Every single life is in danger. No company should be allowed to build a superintelligence.”

### Contacts

- Global, NL: Joep Meindertsma, [joep@ontola.io](mailto:joep@ontola.io), +31636020942
- UK: Joseph Miller, [josephmiller101@gmail.com](mailto:josephmiller101@gmail.com), +44 7943937122

More information about the protest: [pauseai.info/2023-oct](https://pauseai.info/2023-oct) -->
