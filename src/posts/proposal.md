---
title: PauseAI Proposal
description: Implement a temporary pause on the development of AI systems more powerful than GPT-4, ban training on copyrighted material, hold model creators liable.
---

**Implement a temporary pause on the development of AI systems more powerful than GPT-4**.

Individual countries can implement this measure _right now_.
However, we cannot expect countries or companies to risk their competitive advantage by pausing AI development for too long.
This is why we need a global pause.

An international agreement is typically established through a summit, where leaders of countries meet to discuss the issue and make a decision.

We need our leaders to understand the urgency of the situation, and to take action **right now**:

- A country needs to step up and **host a [summit](/summit)**. Pick a date and a location, then invite all UN member states. _Update: The [UK will host](https://www.gov.uk/government/news/uk-to-host-first-global-summit-on-artificial-intelligence) an AI safety summit autumn 2023_
- A **treaty** needs to be created. This treaty should specify what types of AI development are illegal, and which consequences there are to not abiding. This treaty needs to be signed by all UN member states.

## Policy

- **Ban the development and training of AI systems more powerful than GPT-4**, at least until the alignment problem is solved and the safety of such systems can be guaranteed.
  - [**Track the sales of GPUs**](https://arxiv.org/abs/2303.11341) and other hardware that can be used for AI training. This helps to enforce the ban on training.
- **Ban training of AI systems on copyrighted material**. This helps with copyright issues, slows growing inequality and slows down progress towards superintelligence.
- **Hold AI model creators liable** for criminal acts committed using their AI systems. This gives model creators more incentives to make sure their models are safe.
- **Set up an international AI safety agency**, similar to the IAEA. This agency will be responsible for:
  - Periodic meetings to discuss the progress of AI safety research.
  - Granting approval to conduct any new training run above a certain size (e.g. 1 billion parameters).
- **Increase national investments in AI safety research**. Right now, there exist only a few hundred AI safety researchers. This should become thousands.

## Long term policy

At the time of writing, training a GPT-3 sized model costs millions of dollars.
This makes it very difficult for individuals to train such models, and this makes it easier to enforce the ban on training.
However, the cost of training a model is decreasing exponentially.
Hardware improvements and more efficient training algorithms rapidly decrease the cost of training.

There will come a point where potentially superintelligent AI models can be trained for a few thousand dollars or less, perhaps even on consumer hardware.
We need to be prepared for this.
We should consider the following policies:

- **Limit publication of training algorithms**. Sometimes a new algorithm is published that makes training much more efficient. The Transformer architecture, for example, enabled virtually all recent progress in AI. These types of capability jumps can happen at any time, and we should consider limiting the publication of such algorithms to minimize the risk of a sudden capability jump.
- **Limit access to computational resources**. If training a superintelligence becomes possible on consumer hardware, we should consider limiting access to such hardware.
