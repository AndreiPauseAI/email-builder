---
title: Counterarguments
description: As a resource to redirect people to places responding to all kinds of arguments
---

This is a compilation of disagreements about AI dangers and pushing for an AI Pause ordered from the more fundamental to the more specific ones.

## AI is and will be really beneficial to the world

It could be.
But it could also be [dangerous](/risks).
Both aspects have to be taken into account before developing a technology.
And we could understand how to do that safely if we had more time.

## The biggest dangers that you mention aren't true

Some people believe some risks are real, but nothing so extraordinary that could justify a pause.
Someone could believe this for various reasons, and some responses to those beliefs are [here](/ai-x-risk-skepticism).

But, as a short response, the important thing is to realize we are growing alien minds with the objective of making them smarter than us, without fully understanding that process, and in the hands of small subsets of people which we don't really know.

Then, estimate how likely you think that can cause catastrophic events, compare it with [the estimations of experts](/polls-and-surveys#catastrophic-risks-from-ai) and, if you have estimations quite different from them, think about if you can justify that difference.

Lastly, think if those chances of really terrible things happening for developing advanced AI are justified somehow or if we should do something about it.

## I can't make a difference for myself and other arguments

At the heart of the problem, we think that most of the time there are not logical but emotional reasons that get in our way of taking action. In some cases, the arguments could be true, for sure. Like if you have more important things to take care of or better ways to mitigate the dangers. But we believe that's not the case in most cases. [Some actions](/action) are low effort, have a lot of potential, and can be done by pretty much anyone.

You can read about mental obstacles that get in your way of accepting the risks and taking action [here](psychology-of-x-risk). With the help of the community, you can even have more impact and an environment that appreciates your valuable work and incentivizes you to keep going. On the bright side, the neglectedness of the problem also allows you to make a big difference at the start of this promising movement. There's plenty of low-hanging fruit to grab!

## Human extinction? That's just AI companies hyping up their tech

But it's not just AI companies saying it’s an existential threat.
Hundreds of AI scientists have [signed](https://www.safe.ai/work/statement-on-ai-risk) the open letters.
[86%](https://wiki.aiimpacts.org/ai_timelines/predictions_of_human-level_ai_timelines/ai_timeline_surveys/2023_expert_survey_on_progress_in_ai) of AI scientists believe that we could lose control over AI.

## Lose control? AI is just a piece of software, it's designed by humans

Modern AI is not designed, it's trained.
It's quite literally a [digital brain](/digital-brains), consisting of millions of neurons.
A human designs and programs the learning algorithm, but nobody understands the AI that is grown after that.
Even the inventor of that learning algorithm [says]() that nobody has any idea of how these models work

## Well, if it starts doing crazy things, we can just turn it off

Maybe in some cases, but a really smart AI can spread to other machines.
It's just bytes, so it's not bound to one location.

## But then it needs to be able to hack

GPT-4 already can autonomously hack websites.
It already beats 88% of competitive hackers, can autonomously hack 87% of tested vulnerabilities. How smart do you think GPT-6 will be?

## An AI can't interact with the physical world

Well, quite a bit of things are connected to the web. Cars, planes, drones, we now even have humanoid robots. All of these can be hacked. And AI can already autonomously hack websites and [exploit vulnerabilities](/cybersecurity-risk).

And it's not just robots and machines that can be hacked.
People can be influenced too. You can receive a fake phone call, or a fake email. An AI can use other AIs to generate deepfakes. And GPT-4 is already better at persuading people than people are.

## Why would an AI hate humans and want to kill us?

It doesn’t have to be evil or hate humans to be dangerous to humans.
We don’t hate chimpansees, but we still destroy their forests.
We want palm oil, so we take their forest. We’re smarter, so chimps can’t stop us.
An AI might want more compute power to be better at achieving some other goal, so it destroys our environment to build a better computer.

## The AIs that I know don’t have a will of their own - they just do what they’re asked

Even if it has no goals of its own, and it just follows order, someone is going to do something dangerous with it eventually.
There even was a bot called ChaosGPT which was tasked explicitly to do as much as possible to humans.
It was autonomously searching for weapons of mass-destruction on google, but it didn’t get very further than that.
The thing is, the only thing that is protecting us right now is that AI isn’t very smart yet.

## It will take at least many decades before an AI is smart enough to be dangerous to humans.

On Metaculus, [the community prediction for (weak) AGI](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) was 2057 just three years ago, and now it's 2026.

In 2022, AI researchers thought it would take [17 years](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/) until AI would be able to write a New York Times bestseller.
A year later, a Chinese professor [won a writing contest](https://www.scmp.com/news/china/science/article/3245725/chinese-professor-used-ai-write-science-fiction-novel-then-it-won-national-award) with an AI-written book.

We don't know how long we have, but let's err on the side of caution.

Read more about [urgency](/urgency)

## It’s impossible to slow down technology.

We can regulate it by regulating chips.
Training AI models require very specialized hardware, which is only created by one company, TSMC.
That company uses machines that are created by yet another company, ASML.
The supply chain for AI chips is very fragile and can be regulated.

Read more about [feasibility](/feasibility).

## If you ban it here, China will just build it

That’s why we need an [international treaty](https://pauseai.info/proposal).
The same as we have for banning CFCs, or blinding laser weapons.
We can regulate things internationally.

## A Pause would be bad

Some ways in which a pause could be bad and how we could prevent those scenarios are explained on [this page](/mitigating-pause-failures).
But if the article doesn't cover your worries you can tell us about them [here](https://airtable.com/appWPTGqZmUcs3NWu/pagIvo9Sv6IDHaolu/form).

## Nobody wants a Pause

70% of people already believe that governments should pause AI development.
The [popular support](/polls-and-surveys) is already there.
The next step is to let our politicians know that this is urgent.

## I can't make a difference

Yes you can!
There are [many ways](/action) to help, and we need all the help we can get.
