---
title: List of p(doom) values
description: How likely do AI various researchers believe AI will cause human extinction?
---

What is p(doom)?
The probability of very bad outcomes (e.g. human extinction) as a result of AI.

### p(doom) numbers, from low to high

- Yann LeCun (Meta Chief Scientist): [<0.01%](https://twitter.com/liron/status/1736555643384025428)
- Vitalik Buterin: [10%](https://x.com/liron/status/1729740226594258982?s=20)
- Geoff Hinton (one of three godfathers of AI): [10%](https://twitter.com/geoffreyhinton/status/1719447980753719543) chance of extinction in next 30 years if unregulated
- Dario Amodei (CEO of Anthropic): [10-25%](https://twitter.com/liron/status/1710520914444718459) combining something wrong with model plus human misuse
- Emmet Shear [5-50%](https://www.youtube.com/watch?v=9oUbauum4uI)
- Elon Musk: [20-30%](https://www.youtube.com/watch?v=57y7DxWfOS0&t=50s)
- Paul Christiano (former alignment lead at OpenAI, current researcher at ARC) [10-20%](https://www.youtube.com/watch?v=GyFkWb903aU&t=560s) chance of AI takeover, many/most humans dead. Cumulative risks go to 50% when you get to human-level AI
- Jan Leike (alignment lead at OpenAI), Aug 2023: [10-90%](https://www.youtube.com/watch?v=ZP_N4q5U3eE&t=1h16m)
- Lina Khan (head of FTC), Nov 2023: [15%](https://twitter.com/liron/status/1723458202090774949)
- Scott Alexander: [33%](https://astralcodexten.substack.com/p/why-i-am-not-as-much-of-a-doomer)
- Top competitive forecaster Eli Lifland says 35%
- The median machine learning researcher: 5-10% (but in spring 2022, before progress sped up!)
- Average AI engineer: [40%](https://twitter.com/AISafetyMemes/status/1713515843194077583)
- The average person working in AI alignment thinks about 30%
- Holden Karnofsky, on a somewhat related question, gives 50%
- Zvi: [60%](https://x.com/liron/status/1729274710670893262?s=20)
- Dan Hendrycks: [>80%](https://twitter.com/DanHendrycks/status/1642394635657162753)
- Eliezer >95%
