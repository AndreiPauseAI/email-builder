---
title: International PauseAI protest 13th May 2024
description: We are organizing an international protest to demand a pause on dangerous AI development.
---

## May 13th (Saturday), in multiple countries

- **San Francisco**, California (See [Discord](https://discord.com/channels/1100491867675709580/1227035391924768858))
- **Berlin**, Germany (link follows soon)
- **London**, UK (link follows soon)
- **Rome**, Italy (link follows soon)
- **Stockholm**, Sweden (link follows soon)
- **Den Haag**, the Netherlands (link follows soon)

## Why we protest

AI is rapidly becoming more powerful, far faster than virtually any AI scientist has predicted.
Billions are being poured into AI capabilities, and the results are staggering.
New models are [outperforming humans](/sota) in a lot of domains.
As capabilities increase, so do the [risks](/risks).
Scientists are even [warning](https://www.safe.ai/statement-on-ai-risk) that AI might [end up destroying humanity](/xrisk).
This dire outcome not only seems possible but also likely, as the average probability estimates for these outcomes [range from 9% to 40%](/polls-and-surveys).

We need our leaders to listen to these warnings, but they are not taking this topic remotely as seriously as they should.
There is AI safety legislation being drafted, but [not a single measure would prevent or delay superintelligent AI](https://twitter.com/PauseAI/status/1704998018322141496).
[Over 70%](https://www.vox.com/future-perfect/2023/8/18/23836362/ai-slow-down-poll-regulation) of surveyed people want to slow down AI, and [over 60%](https://www.vox.com/future-perfect/2023/9/19/23879648/americans-artificial-general-intelligence-ai-policy-poll) want regulation to actively prevent superintelligent AI.

On May 22nd, the second AI Safety Summit will be held in Seoul, South Korea.
The perfect opportunity to set the first steps toward sensible international AI safety regulation.

## What we ask

- **Policymakers**: Don't allow companies to build a superintelligence. Regulations and hardware restrictions should apply before training has started as it is very difficult to control dissemination once a new capability has been achieved. We cannot allow companies to train potentially world-ending AI models. Writing legislation is hard, and it takes time, but we may not have that long, so work as if your life depends on it. Because it does.
- **Companies**: Many of you are scared of what AI can do, but you're locked in a race. So be vocal about supporting a pause in principle. If you sign statements that this technology could kill us all, show the world that you'd prefer not to build it if it was a viable option.
- **Summit invitees**: Prioritize safety over economic growth. We know AI can make our countries richer, but that's not why you're summoned here. Be the adult in the room.

For our entire proposal, see [here](/proposal).
