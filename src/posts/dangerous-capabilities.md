---
title: Dangerous capabilities in AI
description: The more powerful AI becomes in specific domains, the larger the risks become.
---

As AIs become larger and are trained on more data, they attain new abilities.
It is hard to predict which abilities will appear.
Because of this, they are often called _Emergent Capabilities_.
Some of these capabilities can be extremely dangerous.

## Which capabilities can be dangerous?

- **Cybersecurity**. When an AI is able to discover security vulnerabilities (especially new, unknown ones), it can (be used to) [hack into systems](/cybersecurity-risks).
- **Biological**. When an AI is able to design molecules, it can be used to create biological weapons that are optimized for fatality, virality and resistance to treatment.
- **Algorithmic improvements**. An AI that can find efficient algorithms for a given problem, could lead to a recursive loop of self-improvement, spinning rapidly out of control. This is called an _intelligence explosion_.
- **Deception**. The ability to manipulate people, which includes social engineering. GPT-4 has already shown this capability when it convinced a support desk employee to circumvent a captcha.
- **Self-replication**. If an AI can create new instances on other machines, there is a risk of it spreading uncontrollably. This is called an [_AI takeover_](/ai-takeover).

## How can we prevent dangerous capabilities?

- **Test for dangerous capabilities**. Require that AI systems are tested for dangerous capabilities before they are deployed. External audits with standardized tests could be used to verify that the AI is safe to some extent. However, this is far from a perfect solution for two reasons. Firstly, **some capabilities are hidden** and are only discovered after a long time. Secondly, highly dangerous capabilities are also **dangerous when they are only available to AI labs**.
- **Don't train dangerous AIs**. The safest option is to not build these powerful AI systems in the first place. That's why we're [calling for a Pause](/proposal)!
