---
title: PauseAI protest for UK summit @ Parliament Square - June 8th
description: We are organizing a protest at Parliament Square to demand a summit to pause AI development.
---

- PauseAI protest, calling Rishi Sunak to organise a summit to create an international treaty to implement a pause on AI development.
- Where: Parliament Square, London
- When: 8th of June, 4 PM - 6 PM
- [Sign up form](https://forms.gle/bjWZzJtSnAKqdumh6)

## Press Release

Will be published after the protest.
For questions, send an email to joep@ontola.io.

<!--
On Thursday, June 8th, volunteers from the new [PauseAI](http://pauseai.info) movement will gather in Parliament Square, London to urge the UK government to take the lead on AI safety.
They are asking Rishi Sunak to [organize a summit](https://pauseai.info/summit) to create an international treaty to halt the development of the most capable AI systems.

A rapidly increasing number of AI experts [signed a statement](https://www.safe.ai/statement-on-ai-risk) last week that reads:

> "Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war."

This has been signed by virtually all AI labs (OpenAI, Google Deepmind, Ahthropic) and hundreds of AI scientists.

Rishi Sunak has stated that the ["Government is looking very carefully at this"](https://twitter.com/RishiSunak/status/1663838958558539776) and that ["the UK is well-placed to lead"](https://twitter.com/RishiSunak/status/1662369922234679297) the global collaboration on safe AI development.
The UK is home to some of the world's leading AI labs, including DeepMind, and has a high concentration of AI safety researchers.

The protesters are supporting Rishi Sunak in taking the lead on global AI safety.
They are asking him to [organize a summit](https://pauseai.info/summit) to create an international treaty to halt the development of the most capable AI systems.
This is a different approach from what the AI lab CEOs that Rishi Sunak has spoken with have suggested.
OpenAI believes that ["it would be unintuitively risky and difficult to stop the creation of superintelligence"](https://openai.com/blog/governance-of-superintelligence), so they are pursuing further development towards superintelligence.

> "We have a choice: do we risk human extinction to build a superintelligence, or do we stop while we still can?" - PauseAI protesters

AI safety experts have not reached on consensus on how large the risk of human extinction will be.
Results from the ["Existential risk from AI survey"](https://forum.effectivealtruism.org/posts/8CM9vZ2nnQsWJNsHx/existential-risk-from-ai-survey-results) show that estimates range from 2% to 98%, with an average of 30%.

> "Even a 2% risk of human extinction is unacceptable. At the very least, pausing development deserves to be considered as a viable option, and the decision should be made democratically, not by a few AI lab CEOs." - PauseAI protesters

The protesters believe an international summit to create a treaty to halt the development of the most capable AI systems is the only way in which we can prevent the worst scenarios.

> "We cannot expect nations or companies to halt development, or even to prioritize safety over capabilities, without a global agreement. Both nations and companies are incentivized to develop the most capable AI systems, and they will not stop unless a treaty is in place. One country needs to step up and take the lead in organizing this. Let it be the UK" - PauseAI protesters

For more information, please visit [PauseAI.info](http://pauseai.info). -->

## Contact

- Gideon Futerman ([Twitter](https://twitter.com/GFuterman))
